{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20734dd-3cc8-425f-bf77-715404aac2c5",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b15b27-0bde-4279-bd96-5d6a20a043d0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    GridSearchCV (Grid Search Cross-Validation) is a technique used in machine learning for hyperparameter tuning, which involves finding the best set of hyperparameters for a machine learning model. Hyperparameters are parameters that are not learned by the model during training, but are set by the user before training begins. These parameters can have a significant impact on the performance of the model.\n",
    "\n",
    "The purpose of GridSearchCV is to systematically search through a predefined hyperparameter space to find the combination of hyperparameters that results in the best model performance. It does this by exhaustively trying out all possible combinations of hyperparameters from a specified grid of values and evaluating each combination using cross-validation.\n",
    "\n",
    "Here's how GridSearchCV works:\n",
    "\n",
    "1. **Define Hyperparameter Grid:** You specify a dictionary or a grid of hyperparameter values that you want to search through. For example, if you're tuning the hyperparameters of a support vector machine (SVM), you might define a grid of possible values for the C parameter and the kernel type.\n",
    "\n",
    "2. **Cross-Validation:** GridSearchCV uses k-fold cross-validation to evaluate each combination of hyperparameters. In k-fold cross-validation, the dataset is split into k subsets (folds), and the model is trained and evaluated k times. Each time, one fold is used as the validation set, and the rest are used for training.\n",
    "\n",
    "3. **Model Evaluation:** For each combination of hyperparameters, the model is trained on the training folds and evaluated on the validation fold. The evaluation metric (such as accuracy, F1-score, etc.) is computed for each fold.\n",
    "\n",
    "4. **Aggregation:** The performance metrics from each fold are aggregated (usually by taking the mean or weighted mean) to get an overall performance score for that particular combination of hyperparameters.\n",
    "\n",
    "5. **Best Hyperparameters:** After evaluating all combinations, GridSearchCV identifies the set of hyperparameters that resulted in the best performance according to the chosen evaluation metric.\n",
    "\n",
    "6. **Retraining with Best Hyperparameters:** Once the best hyperparameters are identified, the model is retrained using the entire training dataset, using these optimal hyperparameters.\n",
    "\n",
    "GridSearchCV is a powerful tool for finding the best hyperparameters for your model, but it can be computationally expensive, especially if the hyperparameter search space is large. To address this, there are more advanced techniques like RandomizedSearchCV, which samples a specified number of random combinations from the hyperparameter space, and Bayesian optimization, which uses probabilistic models to guide the search towards promising hyperparameters.\n",
    "\n",
    "In summary, GridSearchCV automates the process of hyperparameter tuning by systematically exploring the hyperparameter space and selecting the combination that leads to the best model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cfe13-5118-4a2e-8d5b-e8d8f3ae7bff",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37de20-41d0-448e-ae22-9a9a49b23a7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    Grid Search CV and Randomized Search CV are both techniques used for hyperparameter tuning in machine learning. They aim to find the best combination of hyperparameters that result in optimal model performance. However, they differ in their approach to exploring the hyperparameter space.\n",
    "\n",
    "**Grid Search CV:**\n",
    "- In Grid Search CV, you define a grid of possible values for each hyperparameter you want to tune.\n",
    "- It exhaustively searches through all possible combinations of hyperparameters within the specified grid.\n",
    "- Grid Search CV evaluates every single combination using cross-validation, which can be computationally expensive.\n",
    "- It guarantees that all combinations within the specified grid will be evaluated.\n",
    "- It's suitable when you have a relatively small search space and want to ensure a comprehensive search across all possible hyperparameter combinations.\n",
    "- Grid Search CV can be time-consuming and memory-intensive, especially when dealing with a large number of hyperparameters or a large dataset.\n",
    "\n",
    "**Randomized Search CV:**\n",
    "- In Randomized Search CV, you define a distribution for each hyperparameter, which guides the sampling process.\n",
    "- It randomly samples a specified number of combinations from the hyperparameter space based on the defined distributions.\n",
    "- Randomized Search CV can be more efficient in terms of computation time compared to Grid Search CV since it samples only a subset of the hyperparameter space.\n",
    "- It may not guarantee that all combinations will be evaluated, but it focuses on exploring regions of the space that are likely to be promising.\n",
    "- Randomized Search CV is suitable when you have a large search space or limited computational resources. It allows you to efficiently explore a wide range of hyperparameters without exhaustively evaluating every combination.\n",
    "- It's also useful when you want to quickly get a sense of which hyperparameters are having a significant impact on model performance.\n",
    "\n",
    "**Choosing Between Grid Search CV and Randomized Search CV:**\n",
    "- Use Grid Search CV when you have a small search space and want to ensure a thorough exploration of all possible hyperparameter combinations.\n",
    "- Use Randomized Search CV when you have a large search space, limited computational resources, or want to quickly identify promising hyperparameters.\n",
    "- If computation time is not a concern and you want a complete search, Grid Search CV might be preferred.\n",
    "- If you need to balance exploration of the hyperparameter space and computational efficiency, Randomized Search CV is a good option.\n",
    "- Consider the nature of your problem, the number of hyperparameters, the size of the dataset, and your available computational resources when deciding between the two.\n",
    "\n",
    "In summary, the choice between Grid Search CV and Randomized Search CV depends on the size of the hyperparameter search space, the available computational resources, and the desired level of exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28504679-8da6-448b-a2cc-b3509c4ffcfb",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46474ff9-ca24-4c8e-a5aa-8633b2b9630b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Data leakage, also known as information leakage, occurs in machine learning when information from outside the training data is inadvertently used to make predictions during model training or evaluation. This can lead to overly optimistic performance metrics and unreliable generalization to new, unseen data. Data leakage can significantly impact the validity and effectiveness of a machine learning model.\n",
    "\n",
    "Data leakage is a problem because it can lead to incorrect assessments of a model's true performance and its ability to generalize to new data. When data leakage occurs, a model might appear to perform exceptionally well on the training data and even on the validation data, but it will likely fail to perform as expected on real-world, unseen data. This undermines the reliability and credibility of the model's predictions.\n",
    "\n",
    "Here's an example of data leakage:\n",
    "\n",
    "**Example: Credit Card Fraud Detection**\n",
    "\n",
    "Imagine you are building a model to detect fraudulent credit card transactions. You have a dataset with transaction records, including features like transaction amount, time, and location, along with a binary target indicating whether a transaction is fraudulent or not.\n",
    "\n",
    "Data Leakage Scenario:\n",
    "\n",
    "1. **Timestamp Leakage:** You accidentally include the exact timestamp of each transaction in your training data. Your model learns to recognize patterns associated with fraudulent transactions, including specific times when fraudulent activity tends to occur (e.g., late at night).\n",
    "2. **Overfitting:** Your model trains on this timestamp information and learns to associate certain timestamps with fraud. During validation, the model seems to perform very well because it's essentially memorized the timestamps associated with fraud in the training data.\n",
    "3. **Test Set Performance:** When you evaluate the model on a test set of new, unseen data, it performs poorly. The model has not learned generalizable patterns; instead, it has learned the specific timestamps from the training data. The timestamps do not generalize to new data, leading to poor performance on real-world fraud detection tasks.\n",
    "\n",
    "In this example, the leakage of timestamp information led to the model learning patterns that are specific to the training data but do not apply to new data. The model's performance was inflated during training and validation, creating a false sense of its effectiveness.\n",
    "\n",
    "To avoid data leakage, it's crucial to follow best practices such as:\n",
    "\n",
    "1. **Feature Selection:** Ensure that only relevant features available at the time of prediction are used in the model. Features that might leak information from the future should be excluded.\n",
    "2. **Cross-Validation:** Use appropriate cross-validation techniques to evaluate model performance in a way that mimics real-world deployment.\n",
    "3. **Feature Engineering:** Be mindful of the features you create and their potential to leak information from the target variable or future data.\n",
    "4. **Temporal Data Handling:** For time-series data, make sure to handle time-related features and information appropriately to prevent leakage.\n",
    "\n",
    "By understanding and mitigating data leakage, you can build more robust and reliable machine learning models that generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66feea06-9950-4780-87df-952fc42cf909",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7f186-17dc-4dc8-8583-7ec644a76c2a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Preventing data leakage is essential to ensure the integrity and reliability of your machine learning model's performance evaluation and generalization to new, unseen data. Here are several strategies and best practices to help prevent data leakage when building a machine learning model:\n",
    "\n",
    "1. **Split Data Properly:**\n",
    "   - Split your dataset into separate subsets for training, validation, and testing.\n",
    "   - Use techniques like k-fold cross-validation for model evaluation to ensure that the model is not overfitting to a specific subset of the data.\n",
    "\n",
    "2. **Feature Selection and Engineering:**\n",
    "   - Only include features in your model that would be available at the time of prediction.\n",
    "   - Avoid using future information or target-related information that could leak information.\n",
    "   - Be cautious with time-related features in time-series data to avoid using information from the future.\n",
    "\n",
    "3. **Temporal Validation:**\n",
    "   - When working with time-series data, perform cross-validation or validation sets in a time-based manner.\n",
    "   - Use past data for training, more recent data for validation, and even more recent data for testing to simulate real-world deployment.\n",
    "\n",
    "4. **Use Proper Techniques for Preprocessing:**\n",
    "   - Standardize, normalize, or transform features based on the training data statistics and apply the same transformations consistently to validation and test data.\n",
    "   - Avoid using information from the validation or test set to compute preprocessing statistics.\n",
    "\n",
    "5. **Holdout Data:**\n",
    "   - Set aside a dedicated holdout dataset that is not used during model development and is only used for final model evaluation.\n",
    "   - This helps provide an unbiased assessment of the model's performance on completely unseen data.\n",
    "\n",
    "6. **Regularization and Hyperparameter Tuning:**\n",
    "   - Use techniques like cross-validation for hyperparameter tuning to avoid using test data in the tuning process.\n",
    "   - Ensure that hyperparameter tuning is performed only using the training data.\n",
    "\n",
    "7. **Categorical Variables:**\n",
    "   - For categorical variables, use techniques like one-hot encoding or target encoding appropriately, avoiding any leakage from validation or test sets.\n",
    "\n",
    "8. **Time-Dependent Data:**\n",
    "   - Be especially cautious with data that has a temporal component, ensuring that time-related information is handled properly.\n",
    "   - Consider features that capture trends or seasonality rather than directly using timestamps.\n",
    "\n",
    "9. **External Data Sources:**\n",
    "   - If incorporating external data sources, ensure that the data is collected and processed independently from your main dataset to prevent information leakage.\n",
    "\n",
    "10. **Regular Monitoring:**\n",
    "    - Continuously monitor your modeling process for potential sources of data leakage, especially if your dataset evolves over time.\n",
    "\n",
    "11. **Domain Knowledge:**\n",
    "    - Leverage domain knowledge to guide your feature selection, engineering, and preprocessing steps, helping to identify and mitigate potential sources of leakage.\n",
    "\n",
    "12. **Documentation and Versioning:**\n",
    "    - Document all preprocessing steps, transformations, and data splits to maintain a clear record of your data handling process.\n",
    "\n",
    "By following these practices, you can significantly reduce the risk of data leakage and build machine learning models that provide accurate and reliable predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad6d65-d794-4e49-8c2b-401d953cad42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369a6e6-e07e-4e6a-a781-9e6e7b9454da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    A confusion matrix is a tabular representation used in classification to evaluate the performance of a machine learning model. It provides a comprehensive view of the model's predictions by breaking down the actual and predicted classes into four categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These categories help assess the model's effectiveness in making accurate predictions and understanding the types of errors it is making.\n",
    "\n",
    "Here's the layout of a confusion matrix:\n",
    "\n",
    "```\n",
    "                    Predicted Positive   Predicted Negative\n",
    "Actual Positive          TP                   FN\n",
    "Actual Negative          FP                   TN\n",
    "```\n",
    "\n",
    "- **True Positives (TP):** These are cases where the model correctly predicted the positive class (e.g., the model predicted a disease, and the individual actually has the disease).\n",
    "\n",
    "- **True Negatives (TN):** These are cases where the model correctly predicted the negative class (e.g., the model predicted a non-disease, and the individual is actually disease-free).\n",
    "\n",
    "- **False Positives (FP):** These are cases where the model incorrectly predicted the positive class when the actual class is negative (e.g., the model predicted a disease, but the individual is actually disease-free).\n",
    "\n",
    "- **False Negatives (FN):** These are cases where the model incorrectly predicted the negative class when the actual class is positive (e.g., the model predicted non-disease, but the individual actually has the disease).\n",
    "\n",
    "The confusion matrix provides valuable insights into the model's performance:\n",
    "\n",
    "1. **Accuracy:** The overall correctness of the model's predictions, calculated as `(TP + TN) / (TP + TN + FP + FN)`. It shows the proportion of correct predictions among all predictions.\n",
    "\n",
    "2. **Precision:** The ability of the model to correctly identify positive cases among the instances it predicts as positive, calculated as `TP / (TP + FP)`. High precision indicates that the model has fewer false positives.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):** The ability of the model to correctly identify positive cases among all actual positive instances, calculated as `TP / (TP + FN)`. High recall indicates that the model has fewer false negatives.\n",
    "\n",
    "4. **Specificity (True Negative Rate):** The ability of the model to correctly identify negative cases among all actual negative instances, calculated as `TN / (TN + FP)`.\n",
    "\n",
    "5. **F1-Score:** The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as `2 * (Precision * Recall) / (Precision + Recall)`.\n",
    "\n",
    "6. **ROC Curve and AUC:** The Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between true positive rate (recall) and false positive rate at various thresholds. The Area Under the ROC Curve (AUC) summarizes the ROC curve's performance in a single metric.\n",
    "\n",
    "The confusion matrix allows you to understand the types of errors your model is making and choose an appropriate evaluation metric based on the specific goals of your classification task. It provides a more nuanced view of a model's performance beyond simple accuracy and helps you assess its strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f20ada-f29f-43f5-b7a1-1a6b1c31c932",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735dfc3-b2af-4756-ac3e-400b4677ea60",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Precision and recall are two important metrics used to evaluate the performance of a classification model, particularly in scenarios where class imbalance or the cost of false positives and false negatives is a concern. They are derived from the confusion matrix, which breaks down the model's predictions into categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "In the context of a confusion matrix, precision and recall are defined as follows:\n",
    "\n",
    "1. **Precision:**\n",
    "   Precision, also known as positive predictive value, measures the proportion of correct positive predictions (true positives) out of all instances predicted as positive (true positives + false positives). It answers the question: \"Of all instances predicted as positive, how many were actually positive?\"\n",
    "\n",
    "   Formula: Precision = TP / (TP + FP)\n",
    "\n",
    "   Precision focuses on the accuracy of positive predictions. A high precision indicates that when the model predicts a positive outcome, it is likely to be correct.\n",
    "\n",
    "2. **Recall:**\n",
    "   Recall, also known as sensitivity or true positive rate, measures the proportion of correct positive predictions (true positives) out of all actual positive instances (true positives + false negatives). It answers the question: \"Of all actual positive instances, how many were correctly predicted as positive?\"\n",
    "\n",
    "   Formula: Recall = TP / (TP + FN)\n",
    "\n",
    "   Recall focuses on the model's ability to capture all positive instances. A high recall indicates that the model is effectively identifying most of the positive cases.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Precision** measures the accuracy of positive predictions made by the model. It is concerned with minimizing false positives.\n",
    "\n",
    "- **Recall** measures the ability of the model to capture all actual positive instances. It is concerned with minimizing false negatives.\n",
    "\n",
    "Choosing between precision and recall depends on the specific goals and requirements of your classification problem:\n",
    "\n",
    "- **High Precision:** If minimizing false positives is crucial (e.g., in medical diagnoses where false positives may lead to unnecessary treatments), prioritize high precision to ensure that when the model predicts a positive, it is highly likely to be correct.\n",
    "\n",
    "- **High Recall:** If capturing as many positive instances as possible is important (e.g., in fraud detection where missing actual fraud cases is costly), prioritize high recall to ensure that the model is effectively identifying most of the positive cases.\n",
    "\n",
    "There is often a trade-off between precision and recall; as one increases, the other may decrease. It's important to strike a balance based on the specific requirements and consequences of false positives and false negatives in your application. The F1-score, which is the harmonic mean of precision and recall, is often used to find a compromise between these two metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110b9e0-5041-4bc7-ab71-fa8dff253745",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba6204-f870-4799-9588-b0df3cfc5fd1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    Interpreting a confusion matrix allows you to gain insight into the types of errors your classification model is making and understand its performance in detail. By analyzing the different components of the confusion matrix, you can identify the specific areas where your model is excelling or struggling. Here's how you can interpret a confusion matrix to determine which types of errors your model is making:\n",
    "\n",
    "Let's recall the layout of a confusion matrix:\n",
    "\n",
    "```\n",
    "                    Predicted Positive   Predicted Negative\n",
    "Actual Positive          TP                   FN\n",
    "Actual Negative          FP                   TN\n",
    "```\n",
    "\n",
    "1. **True Positives (TP):** These are instances where your model correctly predicted the positive class. It indicates how well your model is identifying the positive cases.\n",
    "\n",
    "2. **True Negatives (TN):** These are instances where your model correctly predicted the negative class. It indicates how well your model is identifying the negative cases.\n",
    "\n",
    "3. **False Positives (FP):** These are instances where your model predicted the positive class, but the actual class is negative. This represents Type I errors or false alarms. Analyze the false positives to understand why the model is incorrectly labeling negative instances as positive.\n",
    "\n",
    "4. **False Negatives (FN):** These are instances where your model predicted the negative class, but the actual class is positive. This represents Type II errors or missed opportunities. Analyze the false negatives to understand why the model is failing to capture positive instances.\n",
    "\n",
    "By considering these four components, you can derive insights about the specific errors your model is making:\n",
    "\n",
    "- **High FP and Low FN:** Your model is conservative in predicting positive instances, leading to a high precision. It avoids false positives but might miss some positive cases.\n",
    "\n",
    "- **High FN and Low FP:** Your model is biased towards predicting negative instances, aiming for high recall. It captures most positive instances but might produce more false positives.\n",
    "\n",
    "- **Balanced FP and FN:** Your model is achieving a balance between precision and recall, aiming for an even trade-off between false positives and false negatives.\n",
    "\n",
    "- **High FP and High FN:** Your model is struggling to perform well on both precision and recall, and you need to find ways to improve its overall performance.\n",
    "\n",
    "To further analyze errors, you can consider the following steps:\n",
    "\n",
    "- Examine specific instances that are leading to false positives and false negatives to identify patterns or common characteristics.\n",
    "- Review feature importance or coefficients to understand which features are contributing to misclassifications.\n",
    "- Adjust decision thresholds to optimize precision or recall based on the application's requirements.\n",
    "\n",
    "In summary, interpreting a confusion matrix allows you to diagnose the strengths and weaknesses of your classification model, identify the types of errors it is making, and make informed decisions to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1ae55-8e86-47da-8355-d5d3b96fdd46",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808a87a-6687-4405-83de-51d888a2352a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    Several common metrics can be derived from a confusion matrix to assess the performance of a classification model. These metrics provide insights into different aspects of the model's predictions, such as accuracy, precision, recall, F1-score, specificity, and more. Here's a list of some common metrics and how they are calculated:\n",
    "\n",
    "Let's recall the layout of a confusion matrix:\n",
    "\n",
    "```\n",
    "                    Predicted Positive   Predicted Negative\n",
    "Actual Positive          TP                   FN\n",
    "Actual Negative          FP                   TN\n",
    "```\n",
    "\n",
    "1. **Accuracy:** Overall correctness of the model's predictions.\n",
    "   Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "2. **Precision (Positive Predictive Value):** Proportion of correct positive predictions among all instances predicted as positive.\n",
    "   Formula: Precision = TP / (TP + FP)\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):** Proportion of correct positive predictions among all actual positive instances.\n",
    "   Formula: Recall = TP / (TP + FN)\n",
    "\n",
    "4. **Specificity (True Negative Rate):** Proportion of correct negative predictions among all actual negative instances.\n",
    "   Formula: Specificity = TN / (TN + FP)\n",
    "\n",
    "5. **F1-Score:** Harmonic mean of precision and recall, balancing their trade-off.\n",
    "   Formula: F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "6. **False Positive Rate (FPR):** Proportion of false positive predictions among all actual negative instances.\n",
    "   Formula: FPR = FP / (FP + TN)\n",
    "\n",
    "7. **False Negative Rate (FNR):** Proportion of false negative predictions among all actual positive instances.\n",
    "   Formula: FNR = FN / (FN + TP)\n",
    "\n",
    "8. **Positive Predictive Value (PPV):** Another term for precision.\n",
    "   Formula: PPV = TP / (TP + FP)\n",
    "\n",
    "9. **Negative Predictive Value (NPV):** Proportion of correct negative predictions among all instances predicted as negative.\n",
    "   Formula: NPV = TN / (TN + FN)\n",
    "\n",
    "10. **Matthews Correlation Coefficient (MCC):** Measures the quality of binary classifications, considering all four confusion matrix categories.\n",
    "   Formula: MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "11. **Balanced Accuracy:** Arithmetic mean of sensitivity and specificity, useful when classes are imbalanced.\n",
    "   Formula: Balanced Accuracy = (Sensitivity + Specificity) / 2\n",
    "\n",
    "These metrics provide a comprehensive view of the model's performance, helping you understand its strengths and weaknesses in different aspects of classification. The choice of which metric to emphasize depends on the specific goals and requirements of your application. It's essential to consider the trade-offs between metrics and choose the ones that align with your desired outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25567077-51e2-4992-884f-92aef76aa961",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0c870-9f86-4bf8-a38e-72fc29bd7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de03b71-69d8-4268-bfd5-1df329a62adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
